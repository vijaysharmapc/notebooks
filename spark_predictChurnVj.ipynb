{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, HiveContext\n",
    "import os\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.storagelevel \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pdb\n",
    "import subprocess # Used for executing linux commands, for writing to teradata\n",
    "import sys\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf, min, max\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"state\", StringType(), True),\n",
    "                     StructField(\"account_length\", DoubleType(), True),\n",
    "                     StructField(\"area_code\", StringType(), True),\n",
    "                     StructField(\"phone_number\", StringType(), True),\n",
    "                     StructField(\"international_plan\", StringType(), True),\n",
    "                     StructField(\"voice_mail_plan\", StringType(), True),\n",
    "                     StructField(\"number_vmail_messages\", DoubleType(), True),\n",
    "                     StructField(\"total_day_minutes\", DoubleType(), True),\n",
    "                     StructField(\"total_day_calls\", DoubleType(), True),\n",
    "                     StructField(\"total_day_charge\", DoubleType(), True),\n",
    "                     StructField(\"total_eve_minutes\", DoubleType(), True),\n",
    "                     StructField(\"total_eve_calls\", DoubleType(), True),\n",
    "                     StructField(\"total_eve_charge\", DoubleType(), True),\n",
    "                     StructField(\"total_night_minutes\", DoubleType(), True),\n",
    "                     StructField(\"total_night_calls\", DoubleType(), True),\n",
    "                     StructField(\"total_night_charge\", DoubleType(), True),\n",
    "                     StructField(\"total_intl_minutes\", DoubleType(), True),\n",
    "                     StructField(\"total_intl_calls\", DoubleType(), True),\n",
    "                     StructField(\"total_intl_charge\", DoubleType(), True),\n",
    "                     StructField(\"number_customer_service_calls\", DoubleType(), True),\n",
    "                     StructField(\"churned\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+---------+------------+------------------+---------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+\n",
      "|state|account_length|area_code|phone_number|international_plan|voice_mail_plan|number_vmail_messages|total_day_minutes|total_day_calls|total_day_charge|total_eve_minutes|total_eve_calls|total_eve_charge|total_night_minutes|total_night_calls|total_night_charge|total_intl_minutes|total_intl_calls|total_intl_charge|number_customer_service_calls|churned|\n",
      "+-----+--------------+---------+------------+------------------+---------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+\n",
      "|   KS|         128.0|      415|    382-4657|                no|            yes|                 25.0|            265.1|          110.0|           45.07|            197.4|           99.0|           16.78|              244.7|             91.0|             11.01|              10.0|             3.0|              2.7|                          1.0| False.|\n",
      "|   OH|         107.0|      415|    371-7191|                no|            yes|                 26.0|            161.6|          123.0|           27.47|            195.5|          103.0|           16.62|              254.4|            103.0|             11.45|              13.7|             3.0|              3.7|                          1.0| False.|\n",
      "|   NJ|         137.0|      415|    358-1921|                no|             no|                  0.0|            243.4|          114.0|           41.38|            121.2|          110.0|            10.3|              162.6|            104.0|              7.32|              12.2|             5.0|             3.29|                          0.0| False.|\n",
      "+-----+--------------+---------+------------+------------------+---------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').load('file:///home/vijay/DATA_SCIENCE/ds-for-telco-master/data/churn.all',schema=schema)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  0|  3|\n",
      "+---+---+---+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 0.0, 3.0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what is VectorAssembler?\n",
    "#A feature transformer that merges multiple columns into a vector column.\n",
    "df = spark.createDataFrame([(1, 0, 3)], [\"a\", \"b\", \"c\"])\n",
    "print(df.show())\n",
    "vecAssembler = VectorAssembler(inputCols=[\"a\", \"b\", \"c\"], outputCol=\"features\")\n",
    "vecAssembler.transform(df).head().features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assemble feature vectors\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "        'number_customer_service_calls', \\\n",
    "        'total_night_minutes', \\\n",
    "        'total_day_minutes', \\\n",
    "        'total_eve_minutes', \\\n",
    "        'account_length'],\n",
    "        outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.feature.VectorAssembler"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state=u'KS', account_length=128.0, area_code=u' 415', phone_number=u' 382-4657', international_plan=u' no', voice_mail_plan=u' yes', number_vmail_messages=25.0, total_day_minutes=265.1, total_day_calls=110.0, total_day_charge=45.07, total_eve_minutes=197.4, total_eve_calls=99.0, total_eve_charge=16.78, total_night_minutes=244.7, total_night_calls=91.0, total_night_charge=11.01, total_intl_minutes=10.0, total_intl_calls=3.0, total_intl_charge=2.7, number_customer_service_calls=1.0, churned=u' False.', features=DenseVector([1.0, 244.7, 265.1, 197.4, 128.0]))]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler.transform(df).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature vector is series of numbers containing, one row many columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform labels\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "label_indexer = StringIndexer(inputCol = 'churned', outputCol = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state=u'KS', account_length=128.0, area_code=u' 415', phone_number=u' 382-4657', international_plan=u' no', voice_mail_plan=u' yes', number_vmail_messages=25.0, total_day_minutes=265.1, total_day_calls=110.0, total_day_charge=45.07, total_eve_minutes=197.4, total_eve_calls=99.0, total_eve_charge=16.78, total_night_minutes=244.7, total_night_calls=91.0, total_night_charge=11.01, total_intl_minutes=10.0, total_intl_calls=3.0, total_intl_charge=2.7, number_customer_service_calls=1.0, churned=u' False.', label=0.0)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "#from pyspark.ml import Pipeline?\n",
    "#label_indexer.getParam('label')\n",
    "indexed = label_indexer.fit(df).transform(df)\n",
    "indexed.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init signature: Pipeline(*args, **kwargs)\n",
    "Docstring:     \n",
    "A simple pipeline, which acts as an estimator. A Pipeline consists\n",
    "of a sequence of stages, each of which is either an\n",
    ":py:class:`Estimator` or a :py:class:`Transformer`. When\n",
    ":py:meth:`Pipeline.fit` is called, the stages are executed in\n",
    "order. If a stage is an :py:class:`Estimator`, its\n",
    ":py:meth:`Estimator.fit` method will be called on the input\n",
    "dataset to fit a model. Then the model, which is a transformer,\n",
    "will be used to transform the dataset as the input to the next\n",
    "stage. If a stage is a :py:class:`Transformer`, its\n",
    ":py:meth:`Transformer.transform` method will be called to produce\n",
    "the dataset for the next stage. The fitted model from a\n",
    ":py:class:`Pipeline` is a :py:class:`PipelineModel`, which\n",
    "consists of fitted models and transformers, corresponding to the\n",
    "pipeline stages. If stages is an empty list, the pipeline acts as an\n",
    "identity transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(labelCol = 'label', featuresCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.RandomForestClassifier"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler, label_indexer, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = df.randomSplit([0.6, 0.4])\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8644847307304276"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.transform(train)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7934181226447432"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The AUROC is 0.793418122645 and the AUPR is 0.609153765682.'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictions = model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "auroc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "aupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\"The AUROC is %s and the AUPR is %s.\" % (auroc, aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(state=u'AK', account_length=1.0, area_code=u' 408', phone_number=u' 373-1028', international_plan=u' no', voice_mail_plan=u' no', number_vmail_messages=0.0, total_day_minutes=175.2, total_day_calls=74.0, total_day_charge=29.78, total_eve_minutes=151.7, total_eve_calls=79.0, total_eve_charge=12.89, total_night_minutes=230.5, total_night_calls=109.0, total_night_charge=10.37, total_intl_minutes=5.3, total_intl_calls=3.0, total_intl_charge=1.43, number_customer_service_calls=1.0, churned=u' False.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.861258790887504"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REDO\n",
    "# Assemble feature vectors\n",
    "\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = [\n",
    "        'number_customer_service_calls', \\\n",
    "        'total_night_minutes', \\\n",
    "        'total_day_minutes', \\\n",
    "        'total_eve_minutes', \\\n",
    "        'account_length'],\n",
    "    outputCol = 'churnFeatures')\n",
    "\n",
    "# Transform labels\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "label_indexer = StringIndexer(inputCol = 'churned', outputCol = 'label')\n",
    "# Fit the model\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(labelCol = 'label', featuresCol = 'churnFeatures')\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, label_indexer, classifier])\n",
    "\n",
    "(train, test) = df.randomSplit([0.7, 0.3])\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictions = model.transform(train)\n",
    "print(train.take(1))\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The AUROC is 0.798443360551 and the AUPR is 0.61261371153.'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "auroc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "aupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\"The AUROC is %s and the AUPR is %s.\" % (auroc, aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state=u'IN', account_length=65, area_code=415, phone_number=-6274, international_plan=u'no', voice_mail_plan=u'no', number_vmail_messages=0, total_day_minutes=129.1, total_day_calls=137, total_day_charge=21.95, total_eve_minutes=228.5, total_eve_calls=83, total_eve_charge=19.42, total_night_minutes=208.8, total_night_calls=111, total_night_charge=9.4, total_intl_minutes=12.7, total_intl_calls=6, total_intl_charge=3.43, number_customer_service_calls=4, churnFeatures=DenseVector([4.0, 208.8, 129.1, 228.5, 65.0]), rawPrediction=DenseVector([2.1381, 17.8619]), probability=DenseVector([0.1069, 0.8931]), prediction=1.0)]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state=u'AK', account_length=36.0, area_code=u' 408', phone_number=u' 341-9764', international_plan=u' no', voice_mail_plan=u' yes', number_vmail_messages=30.0, total_day_minutes=146.3, total_day_calls=128.0, total_day_charge=24.87, total_eve_minutes=162.5, total_eve_calls=80.0, total_eve_charge=13.81, total_night_minutes=129.3, total_night_calls=109.0, total_night_charge=5.82, total_intl_minutes=14.5, total_intl_calls=6.0, total_intl_charge=3.92, number_customer_service_calls=0.0, churned=u' False.'),\n",
       " Row(state=u'AK', account_length=36.0, area_code=u' 415', phone_number=u' 399-1526', international_plan=u' yes', voice_mail_plan=u' yes', number_vmail_messages=19.0, total_day_minutes=171.9, total_day_calls=96.0, total_day_charge=29.22, total_eve_minutes=198.4, total_eve_calls=111.0, total_eve_charge=16.86, total_night_minutes=321.7, total_night_calls=76.0, total_night_charge=14.48, total_intl_minutes=10.5, total_intl_calls=1.0, total_intl_charge=2.84, number_customer_service_calls=1.0, churned=u' True.')]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.columns\n",
    "test.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdf = sqlContext.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"feature\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdf1 = sqlContext.createDataFrame([(\"IN\", 65, 415, 329-6603, \"no\", \"no\", 0, 129.1, 137, 21.95, 228.5, 83, 19.42, 208.8, 111, 9.4, 12.7, 6, 3.43, 4)\n",
    "                                  ],['state',\n",
    " 'account_length',\n",
    " 'area_code',\n",
    " 'phone_number',\n",
    " 'international_plan',\n",
    " 'voice_mail_plan',\n",
    " 'number_vmail_messages',\n",
    " 'total_day_minutes',\n",
    " 'total_day_calls',\n",
    " 'total_day_charge',\n",
    " 'total_eve_minutes',\n",
    " 'total_eve_calls',\n",
    " 'total_eve_charge',\n",
    " 'total_night_minutes',\n",
    " 'total_night_calls',\n",
    " 'total_night_charge',\n",
    " 'total_intl_minutes',\n",
    " 'total_intl_calls',\n",
    " 'total_intl_charge',\n",
    " 'number_customer_service_calls'])\n",
    "# notice there is no label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdf2 = sqlContext.createDataFrame([(\"RI\", 74, 415, 344-9403, \"no\", \"no\", 0, 187.7, 127, 31.91, 163.4, 148, 13.89, 196, 94, 8.82, 9.1, 5, 2.46, 0)\n",
    "                                  ],['state',\n",
    " 'account_length',\n",
    " 'area_code',\n",
    " 'phone_number',\n",
    " 'international_plan',\n",
    " 'voice_mail_plan',\n",
    " 'number_vmail_messages',\n",
    " 'total_day_minutes',\n",
    " 'total_day_calls',\n",
    " 'total_day_charge',\n",
    " 'total_eve_minutes',\n",
    " 'total_eve_calls',\n",
    " 'total_eve_charge',\n",
    " 'total_night_minutes',\n",
    " 'total_night_calls',\n",
    " 'total_night_charge',\n",
    " 'total_intl_minutes',\n",
    " 'total_intl_calls',\n",
    " 'total_intl_charge',\n",
    " 'number_customer_service_calls'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(tdf2)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "#auroc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "#aupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "#\"The AUROC is %s and the AUPR is %s.\" % (auroc, aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state=u'RI', account_length=74, area_code=415, phone_number=-9059, international_plan=u'no', voice_mail_plan=u'no', number_vmail_messages=0, total_day_minutes=187.7, total_day_calls=127, total_day_charge=31.91, total_eve_minutes=163.4, total_eve_calls=148, total_eve_charge=13.89, total_night_minutes=196, total_night_calls=94, total_night_charge=8.82, total_intl_minutes=9.1, total_intl_calls=5, total_intl_charge=2.46, number_customer_service_calls=0, churnFeatures=DenseVector([0.0, 196.0, 187.7, 163.4, 74.0]), rawPrediction=DenseVector([18.968, 1.032]), probability=DenseVector([0.9484, 0.0516]), prediction=0.0)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state=u'IN', account_length=65, area_code=415, phone_number=-6274, international_plan=u'no', voice_mail_plan=u'no', number_vmail_messages=0, total_day_minutes=129.1, total_day_calls=137, total_day_charge=21.95, total_eve_minutes=228.5, total_eve_calls=83, total_eve_charge=19.42, total_night_minutes=208.8, total_night_calls=111, total_night_charge=9.4, total_intl_minutes=12.7, total_intl_calls=6, total_intl_charge=3.43, number_customer_service_calls=4, churnFeatures=DenseVector([4.0, 208.8, 129.1, 228.5, 65.0]), rawPrediction=DenseVector([2.1381, 17.8619]), probability=DenseVector([0.1069, 0.8931]), prediction=1.0)]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.transform(tdf1)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "predictions.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IN, 65, 415, 329-6603, no, no, 0, 129.1, 137, 21.95, 228.5, 83, 19.42, 208.8, 111, 9.4, 12.7, 6, 3.43, 4, True.\n",
    "#RI, 74, 415, 344-9403, no, no, 0, 187.7, 127, 31.91, 163.4, 148, 13.89, 196, 94, 8.82, 9.1, 5, 2.46, 0, False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AWESOME STATS WAY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "churn_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+---------+------------+------------------+---------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+\n",
      "|state|account_length|area_code|phone_number|international_plan|voice_mail_plan|number_vmail_messages|total_day_minutes|total_day_calls|total_day_charge|total_eve_minutes|total_eve_calls|total_eve_charge|total_night_minutes|total_night_calls|total_night_charge|total_intl_minutes|total_intl_calls|total_intl_charge|number_customer_service_calls|churned|\n",
      "+-----+--------------+---------+------------+------------------+---------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+\n",
      "|   KS|         128.0|      415|    382-4657|                no|            yes|                 25.0|            265.1|          110.0|           45.07|            197.4|           99.0|           16.78|              244.7|             91.0|             11.01|              10.0|             3.0|              2.7|                          1.0| False.|\n",
      "|   OH|         107.0|      415|    371-7191|                no|            yes|                 26.0|            161.6|          123.0|           27.47|            195.5|          103.0|           16.62|              254.4|            103.0|             11.45|              13.7|             3.0|              3.7|                          1.0| False.|\n",
      "+-----+--------------+---------+------------+------------------+---------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "churn_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function handles categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_category_vars(dataset,field_name): # passing full df and one field\n",
    "    idx_col = field_name + \"Index\"\n",
    "    col_vec = field_name + \"Vec\"\n",
    "    \n",
    "    month_stringIndexer = StringIndexer(inputCol=field_name,\n",
    "                                       outputCol = idx_col)\n",
    "    month_model = month_stringIndexer.fit(dataset)\n",
    "    month_indexed = month_model.transform(dataset)\n",
    "    \n",
    "    month_encoder = OneHotEncoder(dropLast=True,\n",
    "                                 inputCol=idx_col,\n",
    "                                 outputCol = col_vec)\n",
    "    return month_encoder.transform(month_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state',\n",
       " 'account_length',\n",
       " 'area_code',\n",
       " 'phone_number',\n",
       " 'international_plan',\n",
       " 'voice_mail_plan',\n",
       " 'number_vmail_messages',\n",
       " 'total_day_minutes',\n",
       " 'total_day_calls',\n",
       " 'total_day_charge',\n",
       " 'total_eve_minutes',\n",
       " 'total_eve_calls',\n",
       " 'total_eve_charge',\n",
       " 'total_night_minutes',\n",
       " 'total_night_calls',\n",
       " 'total_night_charge',\n",
       " 'total_intl_minutes',\n",
       " 'total_intl_calls',\n",
       " 'total_intl_charge',\n",
       " 'number_customer_service_calls',\n",
       " 'churned']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df_subset = churn_df.select('account_length',\n",
    "                                'international_plan',\n",
    "                                'voice_mail_plan',\n",
    "                                'number_vmail_messages',\n",
    "                                'total_day_minutes',\n",
    "                                'total_eve_minutes',\n",
    "                                'total_night_minutes',\n",
    "                                'total_intl_minutes',\n",
    "                                'number_customer_service_calls',\n",
    "                                'churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['account_length',\n",
       " 'international_plan',\n",
       " 'voice_mail_plan',\n",
       " 'number_vmail_messages',\n",
       " 'total_day_minutes',\n",
       " 'total_eve_minutes',\n",
       " 'total_night_minutes',\n",
       " 'total_intl_minutes',\n",
       " 'number_customer_service_calls',\n",
       " 'churned']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df_subset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(account_length=128.0, international_plan=u' no', voice_mail_plan=u' yes', number_vmail_messages=25.0, total_day_minutes=265.1, total_eve_minutes=197.4, total_night_minutes=244.7, total_intl_minutes=10.0, number_customer_service_calls=1.0, churn=0)]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRANSFORM THE LABELS OR TARGET VARIABLE\n",
    "# EASY WAY TO CONVERT TRUE FALSE TO 0 & 1\n",
    "churn_df_subset = churn_df_subset                       \\\n",
    "  .withColumn( \"churn\",\n",
    "              (churn_df_subset['churned']== 'True.').cast('integer'))\n",
    "\n",
    "churn_df_subset = churn_df_subset.drop('churned')\n",
    "churn_df_subset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create categorical Variables\n",
    "churn_vec = create_category_vars( churn_df_subset,\n",
    "                               \"international_plan\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------+---------------------+\n",
      "|churn|international_planIndex|international_planVec|\n",
      "+-----+-----------------------+---------------------+\n",
      "|    0|                    0.0|        (1,[0],[1.0])|\n",
      "|    0|                    0.0|        (1,[0],[1.0])|\n",
      "+-----+-----------------------+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "churn_vec.select(['churn','international_planIndex','international_planVec']).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "churn_vec = create_category_vars( churn_vec,\n",
    "                               \"voice_mail_plan\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------+---------------------+--------------------+------------------+\n",
      "|churn|international_planIndex|international_planVec|voice_mail_planIndex|voice_mail_planVec|\n",
      "+-----+-----------------------+---------------------+--------------------+------------------+\n",
      "|    0|                    0.0|        (1,[0],[1.0])|                 1.0|         (1,[],[])|\n",
      "|    0|                    0.0|        (1,[0],[1.0])|                 1.0|         (1,[],[])|\n",
      "+-----+-----------------------+---------------------+--------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "churn_vec.select(['churn','international_planIndex','international_planVec','voice_mail_planIndex','voice_mail_planVec']).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "def parseNewPoint( rec ):\n",
    "    return LabeledPoint( float( rec[9] ),\n",
    "          Vectors.dense(tuple( [rec[0],\n",
    "                                rec[3],\n",
    "                                rec[4],\n",
    "                                rec[5],\n",
    "                                rec[6],\n",
    "                                rec[7],\n",
    "                                rec[8] ] +\n",
    "                                rec[11].toArray().tolist() +\n",
    "                                rec[13].toArray().tolist() ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_vec_new = churn_vec.rdd.map( lambda rec:  # map is only available in rdd\n",
    "                            parseNewPoint( rec ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [128.0,25.0,265.1,197.4,244.7,10.0,1.0,1.0,0.0]),\n",
       " LabeledPoint(0.0, [107.0,26.0,161.6,195.5,254.4,13.7,1.0,1.0,0.0])]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_vec_new.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingData, testData = churn_vec_new.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS,    \\\n",
    "                                       LogisticRegressionModel\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_model = LogisticRegressionWithLBFGS.train( trainingData )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labelsAndPreds_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:org.apache.spark.sql.functions.col. Trace:\npy4j.Py4JException: Method col([class java.lang.Integer]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-268-cfba90eb307b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                             rec[0] == rec[1]).count()\n\u001b[1;32m      6\u001b[0m print(\"Successful prediction percentage: \" +\n\u001b[0;32m----> 7\u001b[0;31m     str( round( success_count_lr / labelsAndPreds_lr.count(), 2 ) ) )\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mround\u001b[0;34m(col, scale)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \"\"\"\n\u001b[1;32m    493\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_create_column_from_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n\u001b[1;32m    322\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                     format(target_id, \".\", name, value))\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             raise Py4JError(\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.spark.sql.functions.col. Trace:\npy4j.Py4JException: Method col([class java.lang.Integer]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n"
     ]
    }
   ],
   "source": [
    "labelsAndPreds_lr = testData.map(lambda lp:\n",
    "                               ( float(lr_model.predict(lp.features) ), lp.label ) )\n",
    "\n",
    "success_count_lr = labelsAndPreds_lr.filter(lambda rec:\n",
    "                                            rec[0] == rec[1]).count()\n",
    "print(\"Successful prediction percentage: \" +\n",
    "    str( round( success_count_lr / labelsAndPreds_lr.count(), 2 ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "metrics = MulticlassMetrics( labelsAndPreds_lr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.evaluation.MulticlassMetrics"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def printMetric( metrics ):\n",
    "    print( 'Precision of True ', metrics.precision(1) )\n",
    "\n",
    "    print( 'Precision of False', metrics.precision(0) )\n",
    "    print( 'Recall of True    ', metrics.recall(1) )\n",
    "    print( 'Recall of False   ', metrics.recall(0) )\n",
    "    print( 'F-1 Score         ', metrics.fMeasure() )\n",
    "    print( 'Confusion Matrix\\n', metrics.confusionMatrix().toArray() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3402.precision.\n: java.util.NoSuchElementException: key not found: 1.0\n\tat scala.collection.MapLike$class.default(MapLike.scala:228)\n\tat scala.collection.AbstractMap.default(Map.scala:59)\n\tat scala.collection.mutable.HashMap.apply(HashMap.scala:65)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.precision(MulticlassMetrics.scala:105)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-8b6ec091badb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprintMetric\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-266-ae8445ed0c30>\u001b[0m in \u001b[0;36mprintMetric\u001b[0;34m(metrics)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprintMetric\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Precision of True '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Precision of False'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/mllib/evaluation.py\u001b[0m in \u001b[0;36mprecision\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1.4.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;34m\"\"\"Call method of java_model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3402.precision.\n: java.util.NoSuchElementException: key not found: 1.0\n\tat scala.collection.MapLike$class.default(MapLike.scala:228)\n\tat scala.collection.AbstractMap.default(Map.scala:59)\n\tat scala.collection.mutable.HashMap.apply(HashMap.scala:65)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.precision(MulticlassMetrics.scala:105)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "printMetric( metrics )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
